import os
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix
import numpy as np

# === åŠ è½½æ¨¡å‹ ===
model_path = "../../.cache/modelscope/hub/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-1__5B"
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, trust_remote_code=True).cuda()

# === è¯»å–å¹¶é‡‡æ ·æ•°æ®ï¼ˆæ¯ç±»å„100æ¡ï¼‰===
true_df = pd.read_csv("./fake-and-real/True.csv").dropna(subset=["text"]).sample(n=500, random_state=42)
fake_df = pd.read_csv("./fake-and-real/Fake.csv").dropna(subset=["text"]).sample(n=500, random_state=42)
true_df['label'] = 1
fake_df['label'] = 0

df = pd.concat([true_df, fake_df], ignore_index=True)
df = df[['text', 'label']].dropna()

# === é¢„æµ‹å‡½æ•° ===
def classify_news(text): 
    prompt = f"ä½ æ˜¯ä¸€ä¸ªè™šå‡æ–°é—»è¯†åˆ«ä¸“å®¶ï¼Œè¯·å…ˆå¯¹æ–°é—»å†…å®¹è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼Œå†åˆ¤æ–­ä¸‹é¢è¿™æ®µæ–°é—»æ˜¯çœŸæ–°é—»è¿˜æ˜¯å‡æ–°é—»ï¼Œåªèƒ½ç”¨â€œçœŸâ€æˆ–â€œå‡â€å›ç­”ï¼Œä¸è¦å›ç­”å¤šä½™å†…å®¹ï¼š\n\n{text.strip()}\n\n"
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to(model.device)
    
    with torch.no_grad():
        # å¢åŠ ç”Ÿæˆçš„tokenæ•°é‡ï¼Œå…è®¸ç”Ÿæˆæ›´å¤šæ–‡æœ¬
        outputs = model.generate(
            **inputs,
            max_new_tokens=2,  # å…è®¸æ›´å¤šç”Ÿæˆçš„å†…å®¹
        )
    
    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    answer = output_text.replace(prompt, "").strip()

    # print("=" * 60)
    # print("ğŸ“Œ Prompt:")
    # print(prompt)
    # print("ğŸ¤– æ¨¡å‹å›ç­”:")
    # print(answer)

    if "çœŸ" in answer and "å‡" not in answer:
        return 1
    elif "å‡" in answer and "çœŸ" not in answer:
        return 0
    else:
        return -1

# === æ‰§è¡Œé¢„æµ‹ ===
predictions = []
for text in df['text']:
    pred = classify_news(text)
    # print(pred)
    predictions.append(pred)
df['prediction'] = predictions

# === è¿‡æ»¤æ‰æ— æ³•åˆ¤æ–­çš„æ•°æ® ===
df = df[df['prediction'] != -1]

# === æŒ‡æ ‡è®¡ç®— ===
y_true = df['label'].values
y_pred = df['prediction'].values

accuracy = (y_true == y_pred).mean()
f1 = f1_score(y_true, y_pred)

# confusion_matrix ç»“æ„ï¼š[ [TN, FP], [FN, TP] ]
tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
g_mean = np.sqrt((tp / (tp + fn)) * (tn / (tn + fp)))

try:
    auc = roc_auc_score(y_true, y_pred)
except ValueError:
    auc = float('nan')

# === è¾“å‡ºç»“æœ ===
print("\n" + "=" * 60)
print(f"âœ… æ€»ä½“å‡†ç¡®ç‡ Accuracy: {accuracy:.2%}")
print(f"âœ… F1-score: {f1:.2%}")
print(f"âœ… G-mean: {g_mean:.2%}")
print(f"âœ… AUC: {auc:.2%}")

acc_true = df[df['label'] == 1].apply(lambda row: row['label'] == row['prediction'], axis=1).mean()
acc_fake = df[df['label'] == 0].apply(lambda row: row['label'] == row['prediction'], axis=1).mean()
print(f"ğŸ“˜ çœŸæ–°é—»å‡†ç¡®ç‡ Accuracy_true: {acc_true:.2%}")
print(f"ğŸ“• å‡æ–°é—»å‡†ç¡®ç‡ Accuracy_fake: {acc_fake:.2%}")
