import os
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# === åŠ è½½æ¨¡å‹ ===
model_path = "../../.cache/modelscope/hub/models/Qwen/Qwen2.5-7B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, trust_remote_code=True).cuda()

# === é€‰æ‹©ä¸€ä¸ªæ–°é—»è¿›è¡Œæµ‹è¯• ===
# é€‰æ‹©æ•°æ®ä¸­çš„ä¸€æ¡æ–°é—»
test_df = pd.read_csv("./fake-and-real/True.csv").dropna(subset=["text"]).sample(n=1, random_state=42)  # åªé€‰æ‹©ä¸€æ¡çœŸæ–°é—»
test_text = test_df['text'].iloc[0]  # è·å–æ–°é—»æ–‡æœ¬

# === é¢„æµ‹å‡½æ•° ===
def classify_news(text):
    
    prompt = f"ä½ æ˜¯ä¸€ä¸ªè¯­ä¹‰æƒ…æ„Ÿè¯†åˆ«ä¸“å®¶ï¼Œè¯·åˆ†æä¸‹é¢è¿™æ®µæ–°é—»çš„è¯­ä¹‰æƒ…æ„Ÿï¼š\n\n{text.strip()}\n\n"

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to(model.device)
    
    with torch.no_grad():
        # å¢åŠ ç”Ÿæˆçš„tokenæ•°é‡ï¼Œå…è®¸ç”Ÿæˆæ›´å¤šæ–‡æœ¬
        outputs = model.generate(
            **inputs,
            max_new_tokens=2,  # å…è®¸æ›´å¤šç”Ÿæˆçš„å†…å®¹
        )
    
    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # è¾“å‡ºæ¨¡å‹çš„å›ç­”
    answer = output_text.replace(prompt, "").strip()

    print("=" * 60)
    print("ğŸ“Œ Prompt:")
    print(prompt)
    print("ğŸ¤– æ¨¡å‹å›ç­”:")
    print(answer)

    if "çœŸ" in answer and "å‡" not in answer:
        return 1
    elif "å‡" in answer:
        return 0
    else:
        return -1

# === æµ‹è¯•æ¨¡å‹ ===
pred = classify_news(test_text)
print(pred)