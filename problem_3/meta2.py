import os
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix
import numpy as np

# === åŠ è½½æ¨¡å‹ ===
model_path = "../../.cache/modelscope/hub/models/LLM-Research/Meta-Llama-3-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, trust_remote_code=True).cuda()

# === è¯»å–å¹¶é‡‡æ ·æ•°æ®ï¼ˆæ¯ç±»å„500æ¡ï¼‰===
true_df = pd.read_csv("./fake-and-real/True.csv").dropna(subset=["text"]).sample(n=500, random_state=42)
fake_df = pd.read_csv("./fake-and-real/Fake.csv").dropna(subset=["text"]).sample(n=500, random_state=42)
true_df['label'] = 1
fake_df['label'] = 0

df = pd.concat([true_df, fake_df], ignore_index=True)
df = df[['text', 'label']].dropna()

# === é¢„æµ‹å‡½æ•° ===
def classify_news(text):
    prompt = (f"ä½ æ˜¯ä¸€ä¸ªä¸“æ³¨é€šè¿‡åˆ†ææ–°é—»æƒ…æ„Ÿè¿›è¡Œè¯†åˆ«è™šå‡æ–°é—»çš„ä¸“å®¶ã€‚è¯·é˜…è¯»ä»¥ä¸‹æ–°é—»å†…å®¹ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š\n"
        f"åªèƒ½å›ç­”â€œçœŸâ€æˆ–â€œå‡â€ï¼Œä¸èƒ½åŒ…å«å…¶ä»–ä»»ä½•æ–‡å­—æˆ–è§£é‡Šã€‚\n\n"
        f"æ–°é—»å†…å®¹ï¼š{text.strip()}\n\n"
        f"è¯·å›ç­”ï¼š"
        )
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=1  # å…è®¸ç”Ÿæˆæœ€å¤š2ä¸ªtoken
        )

    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    answer = output_text.replace(prompt, "").strip()
    
    # print(answer)  # å¯é€‰æ‰“å°æ¨¡å‹è¾“å‡º

    if "çœŸ" in answer and "å‡" not in answer:
        return 1
    elif "å‡" in answer and "çœŸ" not in answer:
        return 0
    else:
        return -1

# === æ‰§è¡Œé¢„æµ‹ ===
predictions = []
for text in df['text']:
    pred = classify_news(text)
    predictions.append(pred)

df['prediction'] = predictions

# === è¿‡æ»¤æ‰æ— æ³•åˆ¤æ–­çš„æ•°æ® ===
df = df[df['prediction'] != -1]

# === æŒ‡æ ‡è®¡ç®— ===
y_true = df['label'].values
y_pred = df['prediction'].values
accuracy = (y_true == y_pred).mean()
f1 = f1_score(y_true, y_pred)
tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
g_mean = np.sqrt((tp / (tp + fn)) * (tn / (tn + fp)))

try:
    auc = roc_auc_score(y_true, y_pred)
except ValueError:
    auc = float('nan')

# === è¾“å‡ºç»“æœ ===
print("\n" + "=" * 60)
print(f"âœ… æ€»ä½“å‡†ç¡®ç‡ Accuracy: {accuracy:.2%}")
print(f"âœ… F1-score: {f1:.2%}")
print(f"âœ… G-mean: {g_mean:.2%}")
print(f"âœ… AUC: {auc:.2%}")

acc_true = df[df['label'] == 1].apply(lambda row: row['label'] == row['prediction'], axis=1).mean()
acc_fake = df[df['label'] == 0].apply(lambda row: row['label'] == row['prediction'], axis=1).mean()
print(f"ğŸ“˜ çœŸæ–°é—»å‡†ç¡®ç‡ Accuracy_true: {acc_true:.2%}")
print(f"ğŸ“• å‡æ–°é—»å‡†ç¡®ç‡ Accuracy_fake: {acc_fake:.2%}")
